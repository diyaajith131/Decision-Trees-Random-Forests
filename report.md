# Report: Decision Trees & Random Forests

## 1. Introduction
Decision Trees and Random Forests are tree-based machine learning models widely used for classification and regression.

## 2. Decision Tree
- Works by splitting dataset using features that maximize information gain.
- Evaluated using entropy and Gini index.

## 3. Overfitting & Depth Control
- Deep trees can overfit the training set.
- Limiting tree depth or using pruning helps generalization.

## 4. Random Forest
- An ensemble of decision trees trained using bagging.
- Reduces variance, improves accuracy, and prevents overfitting.

## 5. Feature Importance
Random Forest provides feature importance scores that help interpret the most significant predictors.

## 6. Results
- Decision Tree: Accuracy ~ depends on dataset, prone to overfitting.
- Random Forest: Higher accuracy and more robust.
- Cross-validation confirms Random Forest outperforms a single tree.

## 7. Conclusion
Random Forest is a powerful model compared to a single Decision Tree due to ensemble learning and bagging.
